From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Alex Bradbury <asb@lowrisc.org>
Subject: [RISCV] Lowering for AMO operations

For now this just lowers to AMO+appropriate fence (as indicated in the draft
memory model doc). We should move to using .aq/.rl bits. Also need to ensure
that something sensible is generated for other data widths and AMO ops that
aren't supported natively.
---
 lib/Target/RISCV/RISCVISelLowering.cpp |   7 +-
 lib/Target/RISCV/RISCVInstrInfo.td     |   2 +-
 lib/Target/RISCV/RISCVInstrInfoA.td    |  20 +-
 test/CodeGen/RISCV/atomics.ll          | 461 ++++++++++++++++++++++++-
 4 files changed, 485 insertions(+), 5 deletions(-)

diff --git a/lib/Target/RISCV/RISCVISelLowering.cpp b/lib/Target/RISCV/RISCVISelLowering.cpp
index 12f12bfcacb..943e293836e 100644
--- a/lib/Target/RISCV/RISCVISelLowering.cpp
+++ b/lib/Target/RISCV/RISCVISelLowering.cpp
@@ -1196,8 +1196,7 @@ RISCVTargetLowering::getNumRegistersForCallingConv(LLVMContext &Context,
 Instruction *RISCVTargetLowering::emitLeadingFence(IRBuilder<> &Builder,
                                                    Instruction *Inst,
                                                    AtomicOrdering Ord) const {
-  if (Ord == AtomicOrdering::Release ||
-      Ord == AtomicOrdering::SequentiallyConsistent)
+  if (isReleaseOrStronger(Ord))
     return Builder.CreateFence(Ord);
   return nullptr;
 }
@@ -1205,6 +1204,10 @@ Instruction *RISCVTargetLowering::emitLeadingFence(IRBuilder<> &Builder,
 Instruction *RISCVTargetLowering::emitTrailingFence(IRBuilder<> &Builder,
                                                     Instruction *Inst,
                                                     AtomicOrdering Ord) const {
+  if (Inst->hasAtomicLoad() && Inst->hasAtomicStore() &&
+      (Ord == AtomicOrdering::SequentiallyConsistent ||
+       Ord == AtomicOrdering::AcquireRelease))
+    return Builder.CreateFence(AtomicOrdering::SequentiallyConsistent);
   if (Inst->hasAtomicLoad() && isAcquireOrStronger(Ord))
     return Builder.CreateFence(AtomicOrdering::Acquire);
   return nullptr;
diff --git a/lib/Target/RISCV/RISCVInstrInfo.td b/lib/Target/RISCV/RISCVInstrInfo.td
index 983732db2c8..39092101f5e 100644
--- a/lib/Target/RISCV/RISCVInstrInfo.td
+++ b/lib/Target/RISCV/RISCVInstrInfo.td
@@ -489,7 +489,7 @@ def : InstAlias<"sfence.vma $rs", (SFENCE_VMA GPR:$rs, X0)>;
 
 /// Generic pattern classes
 
-class PatGprGpr<SDPatternOperator OpNode, RVInstR Inst>
+class PatGprGpr<SDPatternOperator OpNode, RVInst Inst>
     : Pat<(OpNode GPR:$rs1, GPR:$rs2), (Inst GPR:$rs1, GPR:$rs2)>;
 class PatGprSimm12<SDPatternOperator OpNode, RVInstI Inst>
     : Pat<(OpNode GPR:$rs1, simm12:$imm12), (Inst GPR:$rs1, simm12:$imm12)>;
diff --git a/lib/Target/RISCV/RISCVInstrInfoA.td b/lib/Target/RISCV/RISCVInstrInfoA.td
index 33e863ba6a1..0e80e1fe6a8 100644
--- a/lib/Target/RISCV/RISCVInstrInfoA.td
+++ b/lib/Target/RISCV/RISCVInstrInfoA.td
@@ -74,4 +74,22 @@ defm AMOMIN_D   : AMO_rr_aq_rl<0b10000, 0b011, "amomin.d">;
 defm AMOMAX_D   : AMO_rr_aq_rl<0b10100, 0b011, "amomax.d">;
 defm AMOMINU_D  : AMO_rr_aq_rl<0b11000, 0b011, "amominu.d">;
 defm AMOMAXU_D  : AMO_rr_aq_rl<0b11100, 0b011, "amomaxu.d">;
-} // Predicates = [HasStedExtA, IsRV64]
+} // Predicates = [HasStdExtA, IsRV64]
+
+//===----------------------------------------------------------------------===//
+// Pseudo-instructions and codegen patterns
+//===----------------------------------------------------------------------===//
+
+let Predicates = [HasStdExtA] in {
+// TODO: Should set .aqrl bits as appropriate rather than doing the
+// fence-based lowering.
+def : PatGprGpr<atomic_swap_32, AMOSWAP_W>;
+def : PatGprGpr<atomic_load_add_32, AMOADD_W>;
+def : PatGprGpr<atomic_load_and_32, AMOAND_W>;
+def : PatGprGpr<atomic_load_or_32, AMOOR_W>;
+def : PatGprGpr<atomic_load_xor_32, AMOXOR_W>;
+def : PatGprGpr<atomic_load_max_32, AMOMAX_W>;
+def : PatGprGpr<atomic_load_min_32, AMOMIN_W>;
+def : PatGprGpr<atomic_load_umax_32, AMOMAXU_W>;
+def : PatGprGpr<atomic_load_umin_32, AMOMINU_W>;
+} // Predicates = [HasStdExtA]
diff --git a/test/CodeGen/RISCV/atomics.ll b/test/CodeGen/RISCV/atomics.ll
index bb81aa9d0eb..5bc59955b32 100644
--- a/test/CodeGen/RISCV/atomics.ll
+++ b/test/CodeGen/RISCV/atomics.ll
@@ -1,5 +1,5 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
-; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
+; RUN: llc -mtriple=riscv32 -mattr=+a -verify-machineinstrs < %s \
 ; RUN:   | FileCheck -check-prefix=RV32I %s
 
 define void @fence_acquire() nounwind {
@@ -268,3 +268,462 @@ define void @atomic_store_i32_seq_cst(i32 *%a, i32 %b) nounwind {
   store atomic i32 %b, i32* %a seq_cst, align 4
   ret void
 }
+
+define i32 @atomicrmw_xchg_i32_monotonic(i32* %a, i32 %b) {
+; RV32I-LABEL: atomicrmw_xchg_i32_monotonic:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amoswap.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw xchg i32* %a, i32 %b monotonic
+  ret i32 %1
+}
+
+define i32 @atomicrmw_xchg_i32_acquire(i32* %a, i32 %b) {
+; RV32I-LABEL: atomicrmw_xchg_i32_acquire:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amoswap.w a0, a1, (a0)
+; RV32I-NEXT:    fence r, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw xchg i32* %a, i32 %b acquire
+  ret i32 %1
+}
+
+define i32 @atomicrmw_xchg_i32_release(i32* %a, i32 %b) {
+; RV32I-LABEL: atomicrmw_xchg_i32_release:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, w
+; RV32I-NEXT:    amoswap.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw xchg i32* %a, i32 %b release
+  ret i32 %1
+}
+
+define i32 @atomicrmw_xchg_i32_acq_rel(i32* %a, i32 %b) {
+; RV32I-LABEL: atomicrmw_xchg_i32_acq_rel:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amoswap.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw xchg i32* %a, i32 %b acq_rel
+  ret i32 %1
+}
+
+define i32 @atomicrmw_xchg_i32_seq_cst(i32* %a, i32 %b) {
+; RV32I-LABEL: atomicrmw_xchg_i32_seq_cst:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amoswap.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw xchg i32* %a, i32 %b seq_cst
+  ret i32 %1
+}
+
+define i32 @atomicrmw_add_i32_monotonic(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_add_i32_monotonic:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amoadd.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw add i32* %a, i32 %b monotonic
+  ret i32 %1
+}
+
+define i32 @atomicrmw_add_i32_acquire(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_add_i32_acquire:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amoadd.w a0, a1, (a0)
+; RV32I-NEXT:    fence r, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw add i32* %a, i32 %b acquire
+  ret i32 %1
+}
+
+define i32 @atomicrmw_add_i32_release(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_add_i32_release:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, w
+; RV32I-NEXT:    amoadd.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw add i32* %a, i32 %b release
+  ret i32 %1
+}
+
+define i32 @atomicrmw_add_i32_acq_rel(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_add_i32_acq_rel:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amoadd.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw add i32* %a, i32 %b acq_rel
+  ret i32 %1
+}
+
+define i32 @atomicrmw_add_i32_seq_cst(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_add_i32_seq_cst:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amoadd.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw add i32* %a, i32 %b seq_cst
+  ret i32 %1
+}
+
+define i32 @atomicrmw_and_i32_monotonic(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_and_i32_monotonic:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amoand.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw and i32* %a, i32 %b monotonic
+  ret i32 %1
+}
+
+define i32 @atomicrmw_and_i32_acquire(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_and_i32_acquire:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amoand.w a0, a1, (a0)
+; RV32I-NEXT:    fence r, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw and i32* %a, i32 %b acquire
+  ret i32 %1
+}
+
+define i32 @atomicrmw_and_i32_release(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_and_i32_release:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, w
+; RV32I-NEXT:    amoand.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw and i32* %a, i32 %b release
+  ret i32 %1
+}
+
+define i32 @atomicrmw_and_i32_acq_rel(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_and_i32_acq_rel:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amoand.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw and i32* %a, i32 %b acq_rel
+  ret i32 %1
+}
+
+define i32 @atomicrmw_and_i32_seq_cst(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_and_i32_seq_cst:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amoand.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw and i32* %a, i32 %b seq_cst
+  ret i32 %1
+}
+
+define i32 @atomicrmw_or_i32_monotonic(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_or_i32_monotonic:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amoor.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw or i32* %a, i32 %b monotonic
+  ret i32 %1
+}
+
+define i32 @atomicrmw_or_i32_acquire(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_or_i32_acquire:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amoor.w a0, a1, (a0)
+; RV32I-NEXT:    fence r, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw or i32* %a, i32 %b acquire
+  ret i32 %1
+}
+
+define i32 @atomicrmw_or_i32_release(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_or_i32_release:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, w
+; RV32I-NEXT:    amoor.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw or i32* %a, i32 %b release
+  ret i32 %1
+}
+
+define i32 @atomicrmw_or_i32_acq_rel(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_or_i32_acq_rel:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amoor.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw or i32* %a, i32 %b acq_rel
+  ret i32 %1
+}
+
+define i32 @atomicrmw_or_i32_seq_cst(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_or_i32_seq_cst:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amoor.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw or i32* %a, i32 %b seq_cst
+  ret i32 %1
+}
+
+define i32 @atomicrmw_xor_i32_monotonic(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_xor_i32_monotonic:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amoxor.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw xor i32* %a, i32 %b monotonic
+  ret i32 %1
+}
+
+define i32 @atomicrmw_xor_i32_acquire(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_xor_i32_acquire:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amoxor.w a0, a1, (a0)
+; RV32I-NEXT:    fence r, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw xor i32* %a, i32 %b acquire
+  ret i32 %1
+}
+
+define i32 @atomicrmw_xor_i32_release(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_xor_i32_release:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, w
+; RV32I-NEXT:    amoxor.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw xor i32* %a, i32 %b release
+  ret i32 %1
+}
+
+define i32 @atomicrmw_xor_i32_acq_rel(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_xor_i32_acq_rel:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amoxor.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw xor i32* %a, i32 %b acq_rel
+  ret i32 %1
+}
+
+define i32 @atomicrmw_xor_i32_seq_cst(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_xor_i32_seq_cst:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amoxor.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw xor i32* %a, i32 %b seq_cst
+  ret i32 %1
+}
+
+define i32 @atomicrmw_max_i32_monotonic(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_max_i32_monotonic:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amomax.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw max i32* %a, i32 %b monotonic
+  ret i32 %1
+}
+
+define i32 @atomicrmw_max_i32_acquire(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_max_i32_acquire:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amomax.w a0, a1, (a0)
+; RV32I-NEXT:    fence r, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw max i32* %a, i32 %b acquire
+  ret i32 %1
+}
+
+define i32 @atomicrmw_max_i32_release(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_max_i32_release:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, w
+; RV32I-NEXT:    amomax.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw max i32* %a, i32 %b release
+  ret i32 %1
+}
+
+define i32 @atomicrmw_max_i32_acq_rel(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_max_i32_acq_rel:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amomax.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw max i32* %a, i32 %b acq_rel
+  ret i32 %1
+}
+
+define i32 @atomicrmw_max_i32_seq_cst(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_max_i32_seq_cst:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amomax.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw max i32* %a, i32 %b seq_cst
+  ret i32 %1
+}
+
+define i32 @atomicrmw_min_i32_monotonic(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_min_i32_monotonic:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amomin.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw min i32* %a, i32 %b monotonic
+  ret i32 %1
+}
+
+define i32 @atomicrmw_min_i32_acquire(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_min_i32_acquire:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amomin.w a0, a1, (a0)
+; RV32I-NEXT:    fence r, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw min i32* %a, i32 %b acquire
+  ret i32 %1
+}
+
+define i32 @atomicrmw_min_i32_release(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_min_i32_release:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, w
+; RV32I-NEXT:    amomin.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw min i32* %a, i32 %b release
+  ret i32 %1
+}
+
+define i32 @atomicrmw_min_i32_acq_rel(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_min_i32_acq_rel:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amomin.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw min i32* %a, i32 %b acq_rel
+  ret i32 %1
+}
+
+define i32 @atomicrmw_min_i32_seq_cst(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_min_i32_seq_cst:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amomin.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw min i32* %a, i32 %b seq_cst
+  ret i32 %1
+}
+
+define i32 @atomicrmw_umax_i32_monotonic(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_umax_i32_monotonic:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amomaxu.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw umax i32* %a, i32 %b monotonic
+  ret i32 %1
+}
+
+define i32 @atomicrmw_umax_i32_acquire(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_umax_i32_acquire:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amomaxu.w a0, a1, (a0)
+; RV32I-NEXT:    fence r, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw umax i32* %a, i32 %b acquire
+  ret i32 %1
+}
+
+define i32 @atomicrmw_umax_i32_release(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_umax_i32_release:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, w
+; RV32I-NEXT:    amomaxu.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw umax i32* %a, i32 %b release
+  ret i32 %1
+}
+
+define i32 @atomicrmw_umax_i32_acq_rel(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_umax_i32_acq_rel:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amomaxu.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw umax i32* %a, i32 %b acq_rel
+  ret i32 %1
+}
+
+define i32 @atomicrmw_umax_i32_seq_cst(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_umax_i32_seq_cst:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amomaxu.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw umax i32* %a, i32 %b seq_cst
+  ret i32 %1
+}
+
+define i32 @atomicrmw_umin_i32_monotonic(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_umin_i32_monotonic:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amominu.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw umin i32* %a, i32 %b monotonic
+  ret i32 %1
+}
+
+define i32 @atomicrmw_umin_i32_acquire(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_umin_i32_acquire:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    amominu.w a0, a1, (a0)
+; RV32I-NEXT:    fence r, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw umin i32* %a, i32 %b acquire
+  ret i32 %1
+}
+
+define i32 @atomicrmw_umin_i32_release(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_umin_i32_release:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, w
+; RV32I-NEXT:    amominu.w a0, a1, (a0)
+; RV32I-NEXT:    ret
+  %1 = atomicrmw umin i32* %a, i32 %b release
+  ret i32 %1
+}
+
+define i32 @atomicrmw_umin_i32_acq_rel(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_umin_i32_acq_rel:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amominu.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw umin i32* %a, i32 %b acq_rel
+  ret i32 %1
+}
+
+define i32 @atomicrmw_umin_i32_seq_cst(i32 *%a, i32 %b) nounwind {
+; RV32I-LABEL: atomicrmw_umin_i32_seq_cst:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    amominu.w a0, a1, (a0)
+; RV32I-NEXT:    fence rw, rw
+; RV32I-NEXT:    ret
+  %1 = atomicrmw umin i32* %a, i32 %b seq_cst
+  ret i32 %1
+}
-- 
2.17.2

