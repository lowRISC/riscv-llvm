From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Alex Bradbury <asb@lowrisc.org>
Subject: [RISCV] Add initial RV64I codegen support

---
 lib/Target/RISCV/RISCV.td                 |   1 +
 lib/Target/RISCV/RISCVISelLowering.cpp    |   6 +-
 lib/Target/RISCV/RISCVInstrInfo.cpp       |  24 +-
 lib/Target/RISCV/RISCVInstrInfo.h         |   5 +-
 lib/Target/RISCV/RISCVInstrInfo.td        |  67 ++-
 lib/Target/RISCV/RISCVRegisterInfo.cpp    |   1 +
 lib/Target/RISCV/RISCVSubtarget.cpp       |   2 +-
 test/CodeGen/RISCV/addc-adde-sube-subc.ll | 100 +++++
 test/CodeGen/RISCV/alu32.ll               | 150 +++++++
 test/CodeGen/RISCV/alu64.ll               | 484 ++++++++++++++++++++++
 test/CodeGen/RISCV/imm.ll                 |  64 +++
 test/CodeGen/RISCV/mem64.ll               | 229 ++++++++++
 12 files changed, 1114 insertions(+), 19 deletions(-)
 create mode 100644 test/CodeGen/RISCV/alu64.ll
 create mode 100644 test/CodeGen/RISCV/mem64.ll

diff --git a/lib/Target/RISCV/RISCV.td b/lib/Target/RISCV/RISCV.td
index e3cf8afcf00..170c8ff2825 100644
--- a/lib/Target/RISCV/RISCV.td
+++ b/lib/Target/RISCV/RISCV.td
@@ -42,6 +42,7 @@ def Feature64Bit
     : SubtargetFeature<"64bit", "HasRV64", "true", "Implements RV64">;
 def IsRV64 : Predicate<"Subtarget->is64Bit()">,
                        AssemblerPredicate<"Feature64Bit">;
+def IsRV32 : Predicate<"!Subtarget->is64Bit()">;
 
 def RV64 : HwMode<"+64bit">;
 def RV32 : HwMode<"-64bit">;
diff --git a/lib/Target/RISCV/RISCVISelLowering.cpp b/lib/Target/RISCV/RISCVISelLowering.cpp
index 3615418ac57..b80f4aca6bd 100644
--- a/lib/Target/RISCV/RISCVISelLowering.cpp
+++ b/lib/Target/RISCV/RISCVISelLowering.cpp
@@ -183,7 +183,7 @@ SDValue RISCVTargetLowering::lowerGlobalAddress(SDValue Op,
   const GlobalValue *GV = N->getGlobal();
   int64_t Offset = N->getOffset();
 
-  if (isPositionIndependent() || Subtarget.is64Bit())
+  if (isPositionIndependent())
     report_fatal_error("Unable to lowerGlobalAddress");
 
   SDValue GAHi =
@@ -204,7 +204,7 @@ SDValue RISCVTargetLowering::lowerBlockAddress(SDValue Op,
   const BlockAddress *BA = N->getBlockAddress();
   int64_t Offset = N->getOffset();
 
-  if (isPositionIndependent() || Subtarget.is64Bit())
+  if (isPositionIndependent())
     report_fatal_error("Unable to lowerBlockAddress");
 
   SDValue BAHi = DAG.getTargetBlockAddress(BA, Ty, Offset, RISCVII::MO_HI);
@@ -224,7 +224,7 @@ SDValue RISCVTargetLowering::lowerExternalSymbol(SDValue Op,
 
   // TODO: should also handle gp-relative loads.
 
-  if (isPositionIndependent() || Subtarget.is64Bit())
+  if (isPositionIndependent())
     report_fatal_error("Unable to lowerExternalSymbol");
 
   SDValue GAHi = DAG.getTargetExternalSymbol(Sym, Ty, RISCVII::MO_HI);
diff --git a/lib/Target/RISCV/RISCVInstrInfo.cpp b/lib/Target/RISCV/RISCVInstrInfo.cpp
index 6a10329d4b8..e2fc3ad91e0 100644
--- a/lib/Target/RISCV/RISCVInstrInfo.cpp
+++ b/lib/Target/RISCV/RISCVInstrInfo.cpp
@@ -29,8 +29,9 @@
 
 using namespace llvm;
 
-RISCVInstrInfo::RISCVInstrInfo()
-    : RISCVGenInstrInfo(RISCV::ADJCALLSTACKDOWN, RISCV::ADJCALLSTACKUP) {}
+RISCVInstrInfo::RISCVInstrInfo(const RISCVSubtarget &Subtarget)
+    : RISCVGenInstrInfo(RISCV::ADJCALLSTACKDOWN, RISCV::ADJCALLSTACKUP),
+      Subtarget(Subtarget) {}
 
 void RISCVInstrInfo::copyPhysReg(MachineBasicBlock &MBB,
                                  MachineBasicBlock::iterator MBBI,
@@ -53,13 +54,15 @@ void RISCVInstrInfo::storeRegToStackSlot(MachineBasicBlock &MBB,
   if (I != MBB.end())
     DL = I->getDebugLoc();
 
-  if (RISCV::GPRRegClass.hasSubClassEq(RC))
-    BuildMI(MBB, I, DL, get(RISCV::SW))
+  if (RISCV::GPRRegClass.hasSubClassEq(RC)) {
+    unsigned Opcode = Subtarget.is64Bit() ? RISCV::SD : RISCV::SW;
+    BuildMI(MBB, I, DL, get(Opcode))
         .addReg(SrcReg, getKillRegState(IsKill))
         .addFrameIndex(FI)
         .addImm(0);
-  else
+  } else {
     llvm_unreachable("Can't store this register to stack slot");
+  }
 }
 
 void RISCVInstrInfo::loadRegFromStackSlot(MachineBasicBlock &MBB,
@@ -71,10 +74,12 @@ void RISCVInstrInfo::loadRegFromStackSlot(MachineBasicBlock &MBB,
   if (I != MBB.end())
     DL = I->getDebugLoc();
 
-  if (RISCV::GPRRegClass.hasSubClassEq(RC))
-    BuildMI(MBB, I, DL, get(RISCV::LW), DstReg).addFrameIndex(FI).addImm(0);
-  else
+  if (RISCV::GPRRegClass.hasSubClassEq(RC)) {
+    unsigned Opcode = Subtarget.is64Bit() ? RISCV::LD : RISCV::LW;
+    BuildMI(MBB, I, DL, get(Opcode), DstReg).addFrameIndex(FI).addImm(0);
+  } else {
     llvm_unreachable("Can't load this register from stack slot");
+  }
 }
 
 void RISCVInstrInfo::movImm32(MachineBasicBlock &MBB,
@@ -282,9 +287,8 @@ unsigned RISCVInstrInfo::insertIndirectBranch(MachineBasicBlock &MBB,
   MachineFunction *MF = MBB.getParent();
   MachineRegisterInfo &MRI = MF->getRegInfo();
   const auto &TM = static_cast<const RISCVTargetMachine &>(MF->getTarget());
-  const auto &STI = MF->getSubtarget<RISCVSubtarget>();
 
-  if (TM.isPositionIndependent() || STI.is64Bit())
+  if (TM.isPositionIndependent())
     report_fatal_error("Unable to insert indirect branch");
 
   if (!isInt<32>(BrOffset))
diff --git a/lib/Target/RISCV/RISCVInstrInfo.h b/lib/Target/RISCV/RISCVInstrInfo.h
index 5761d9bedd7..c7c91b64099 100644
--- a/lib/Target/RISCV/RISCVInstrInfo.h
+++ b/lib/Target/RISCV/RISCVInstrInfo.h
@@ -22,10 +22,13 @@
 
 namespace llvm {
 
+class RISCVSubtarget;
+
 class RISCVInstrInfo : public RISCVGenInstrInfo {
+  const RISCVSubtarget &Subtarget;
 
 public:
-  RISCVInstrInfo();
+  explicit RISCVInstrInfo(const RISCVSubtarget &Subtarget);
 
   void copyPhysReg(MachineBasicBlock &MBB, MachineBasicBlock::iterator MBBI,
                    const DebugLoc &DL, unsigned DstReg, unsigned SrcReg,
diff --git a/lib/Target/RISCV/RISCVInstrInfo.td b/lib/Target/RISCV/RISCVInstrInfo.td
index b90a7ef3176..8eb2cdaff1f 100644
--- a/lib/Target/RISCV/RISCVInstrInfo.td
+++ b/lib/Target/RISCV/RISCVInstrInfo.td
@@ -26,7 +26,6 @@ def SDT_RISCVSelectCC     : SDTypeProfile<1, 5, [SDTCisSameAs<1, 2>,
                                                  SDTCisSameAs<0, 4>,
                                                  SDTCisSameAs<4, 5>]>;
 
-
 def Call         : SDNode<"RISCVISD::CALL", SDT_RISCVCall,
                           [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue,
                            SDNPVariadic]>;
@@ -126,6 +125,7 @@ def ixlenimm : Operand<XLenVT>;
 
 // Standalone (codegen-only) immleaf patterns.
 def simm32 : ImmLeaf<XLenVT, [{return isInt<32>(Imm);}]>;
+def simm64 : ImmLeaf<XLenVT, [{return isInt<64>(Imm);}]>;
 
 // Addressing modes.
 // Necessary because a frameindex can't be matched directly in a pattern.
@@ -146,6 +146,23 @@ def HI20 : SDNodeXForm<imm, [{
                                    SDLoc(N), N->getValueType(0));
 }]>;
 
+// Extract least significant 12 bits from the upper half of an immediate value
+// and sign extend them.
+def HLO12Sext : SDNodeXForm<imm, [{
+  uint64_t Val = N->getZExtValue() >> 32;
+  return CurDAG->getTargetConstant(SignExtend64<12>(Val),
+                                   SDLoc(N), N->getValueType(0));
+}]>;
+
+// Extract the most significant 20 bits from the upper half of an immediate
+// value. Add 1 if bit 32+11 is 1, to compensate for the low 12 bits in the
+// matching immediate addi or ld/st being negative.
+def HHI20 : SDNodeXForm<imm, [{
+  uint64_t Val = N->getZExtValue() >> 32;
+  return CurDAG->getTargetConstant(((Val+0x800) >> 12) & 0xfffff,
+                                   SDLoc(N), N->getValueType(0));
+}]>;
+
 //===----------------------------------------------------------------------===//
 // Instruction Class Templates
 //===----------------------------------------------------------------------===//
@@ -361,7 +378,8 @@ def IsOrAdd: PatFrag<(ops node:$A, node:$B), (or node:$A, node:$B), [{
 
 def : Pat<(simm12:$imm), (ADDI X0, simm12:$imm)>;
 // TODO: Add a pattern for immediates with all zeroes in the lower 12 bits.
-def : Pat<(simm32:$imm), (ADDI (LUI (HI20 imm:$imm)), (LO12Sext imm:$imm))>;
+def : Pat<(simm32:$imm), (ADDI (LUI (HI20 imm:$imm)), (LO12Sext imm:$imm))>,
+      Requires<[IsRV32]>;
 
 /// Simple arithmetic operations
 
@@ -481,7 +499,7 @@ defm : LdPat<sextloadi8, LB>;
 defm : LdPat<extloadi8, LB>;
 defm : LdPat<sextloadi16, LH>;
 defm : LdPat<extloadi16, LH>;
-defm : LdPat<load, LW>;
+defm : LdPat<load, LW>, Requires<[IsRV32]>;
 defm : LdPat<zextloadi8, LBU>;
 defm : LdPat<zextloadi16, LHU>;
 
@@ -500,7 +518,7 @@ multiclass StPat<PatFrag StoreOp, RVInst Inst> {
 
 defm : StPat<truncstorei8, SB>;
 defm : StPat<truncstorei16, SH>;
-defm : StPat<store, SW>;
+defm : StPat<store, SW>, Requires<[IsRV32]>;
 
 /// Other pseudo-instructions
 
@@ -512,6 +530,47 @@ def ADJCALLSTACKUP   : Pseudo<(outs), (ins i32imm:$amt1, i32imm:$amt2),
                               [(CallSeqEnd timm:$amt1, timm:$amt2)]>;
 } // Defs = [X2], Uses = [X2]
 
+/// RV64 patterns
+
+let Predicates = [IsRV64] in {
+def : Pat<(simm32:$imm), (ADDIW (LUI (HI20 imm:$imm)), (LO12Sext imm:$imm))>;
+// TODO: add better patterns for loading 64-bit immediates
+def : Pat<(simm64:$imm), (OR (SLLI (ADDIW (LUI (HHI20 imm:$imm)),
+                                           (HLO12Sext imm:$imm)),
+                                   (i64 32)),
+                             (SRLI (SLLI (ADDIW (LUI (HI20 imm:$imm)),
+                                          (LO12Sext imm:$imm)),
+                                    32),
+                              32))>;
+
+def : Pat<(sext_inreg (add GPR:$rs1, simm12:$imm12), i32),
+          (ADDIW GPR:$rs1, simm12:$imm12)>;
+// sext.w
+def : Pat<(sext_inreg GPR:$rs1, i32), (ADDIW GPR:$rs1, 0)>;
+
+def : Pat<(sext_inreg (shl GPR:$rs1, uimm5:$shamt), i32),
+          (SLLIW GPR:$rs1, uimm5:$shamt)>;
+// TODO: Patterns for SRLIW, SRAIW
+def : Pat<(sext_inreg (add GPR:$rs1, GPR:$rs2), i32),
+          (ADDW GPR:$rs1, GPR:$rs2)>;
+def : Pat<(sext_inreg (sub GPR:$rs1, GPR:$rs2), i32),
+          (SUBW GPR:$rs1, GPR:$rs2)>;
+def : Pat<(sext_inreg (shl GPR:$rs1, GPR:$rs2), i32),
+          (SLLW GPR:$rs1, GPR:$rs2)>;
+def : Pat<(srl (and GPR:$rs1, 0xffffffff), GPR:$rs2),
+          (SRLW GPR:$rs1, GPR:$rs2)>;
+def : Pat<(sra (sext_inreg GPR:$rs1, i32), GPR:$rs2),
+          (SRAW GPR:$rs1, GPR:$rs2)>;
+
+defm : LdPat<sextloadi32, LW>;
+defm : LdPat<extloadi32, LW>;
+defm : LdPat<zextloadi32, LWU>;
+defm : LdPat<load, LD>;
+
+defm : StPat<truncstorei32, SW>;
+defm : StPat<store, SD>;
+} // Predicates = [IsRV64]
+
 //===----------------------------------------------------------------------===//
 // Standard extensions
 //===----------------------------------------------------------------------===//
diff --git a/lib/Target/RISCV/RISCVRegisterInfo.cpp b/lib/Target/RISCV/RISCVRegisterInfo.cpp
index 6ad8bf7bca0..ca2d11271d4 100644
--- a/lib/Target/RISCV/RISCVRegisterInfo.cpp
+++ b/lib/Target/RISCV/RISCVRegisterInfo.cpp
@@ -61,6 +61,7 @@ void RISCVRegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
 
   MachineInstr &MI = *II;
   MachineFunction &MF = *MI.getParent()->getParent();
+  const auto &Subtarget = MF.getSubtarget<RISCVSubtarget>();
   MachineRegisterInfo &MRI = MF.getRegInfo();
   const RISCVInstrInfo *TII = MF.getSubtarget<RISCVSubtarget>().getInstrInfo();
   DebugLoc DL = MI.getDebugLoc();
diff --git a/lib/Target/RISCV/RISCVSubtarget.cpp b/lib/Target/RISCV/RISCVSubtarget.cpp
index b221ea84a33..f06a068eaca 100644
--- a/lib/Target/RISCV/RISCVSubtarget.cpp
+++ b/lib/Target/RISCV/RISCVSubtarget.cpp
@@ -45,4 +45,4 @@ RISCVSubtarget::RISCVSubtarget(const Triple &TT, const std::string &CPU,
                                const std::string &FS, const TargetMachine &TM)
     : RISCVGenSubtargetInfo(TT, CPU, FS),
       FrameLowering(initializeSubtargetDependencies(CPU, FS, TT.isArch64Bit())),
-      InstrInfo(), RegInfo(getHwMode()), TLInfo(TM, *this) {}
+      InstrInfo(*this), RegInfo(getHwMode()), TLInfo(TM, *this) {}
diff --git a/test/CodeGen/RISCV/addc-adde-sube-subc.ll b/test/CodeGen/RISCV/addc-adde-sube-subc.ll
index 54f5482b9e7..0fd223534aa 100644
--- a/test/CodeGen/RISCV/addc-adde-sube-subc.ll
+++ b/test/CodeGen/RISCV/addc-adde-sube-subc.ll
@@ -1,6 +1,8 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
 ; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
 ; RUN:   | FileCheck -check-prefix=RV32I %s
+; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s \
+; RUN:   | FileCheck -check-prefix=RV64I %s
 
 ; Ensure that the ISDOpcodes ADDC, ADDE, SUBC, SUBE are handled correctly
 
@@ -13,6 +15,11 @@ define i64 @addc_adde(i64 %a, i64 %b) {
 ; RV32I-NEXT:    add a1, a1, a0
 ; RV32I-NEXT:    addi a0, a2, 0
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: addc_adde:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    add a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = add i64 %a, %b
   ret i64 %1
 }
@@ -25,6 +32,99 @@ define i64 @subc_sube(i64 %a, i64 %b) {
 ; RV32I-NEXT:    sub a1, a1, a3
 ; RV32I-NEXT:    sub a0, a0, a2
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: subc_sube:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sub a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = sub i64 %a, %b
   ret i64 %1
 }
+
+define i128 @addc_adde128(i128 %a, i128 %b) {
+; RV32I-LABEL: addc_adde128:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    lw a3, 4(a2)
+; RV32I-NEXT:    lw a5, 4(a1)
+; RV32I-NEXT:    add a6, a5, a3
+; RV32I-NEXT:    lw a3, 0(a2)
+; RV32I-NEXT:    lw a7, 0(a1)
+; RV32I-NEXT:    add a4, a7, a3
+; RV32I-NEXT:    sltu a3, a4, a7
+; RV32I-NEXT:    add a6, a6, a3
+; RV32I-NEXT:    beq a6, a5, .LBB2_2
+; RV32I-NEXT:  # %bb.1:
+; RV32I-NEXT:    sltu a3, a6, a5
+; RV32I-NEXT:  .LBB2_2:
+; RV32I-NEXT:    sw a4, 0(a0)
+; RV32I-NEXT:    sw a6, 4(a0)
+; RV32I-NEXT:    lw a4, 12(a2)
+; RV32I-NEXT:    lw a5, 12(a1)
+; RV32I-NEXT:    add a4, a5, a4
+; RV32I-NEXT:    lw a2, 8(a2)
+; RV32I-NEXT:    lw a1, 8(a1)
+; RV32I-NEXT:    add a2, a1, a2
+; RV32I-NEXT:    add a3, a2, a3
+; RV32I-NEXT:    sw a3, 8(a0)
+; RV32I-NEXT:    sltu a3, a3, a2
+; RV32I-NEXT:    sltu a1, a2, a1
+; RV32I-NEXT:    add a1, a4, a1
+; RV32I-NEXT:    add a1, a1, a3
+; RV32I-NEXT:    sw a1, 12(a0)
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: addc_adde128:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    add a1, a1, a3
+; RV64I-NEXT:    add a2, a0, a2
+; RV64I-NEXT:    sltu a0, a2, a0
+; RV64I-NEXT:    add a1, a1, a0
+; RV64I-NEXT:    addi a0, a2, 0
+; RV64I-NEXT:    jalr zero, ra, 0
+  %1 = add i128 %a, %b
+  ret i128 %1
+}
+
+define i128 @subc_sube128(i128 %a, i128 %b) {
+; RV32I-LABEL: subc_sube128:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    lw a4, 4(a2)
+; RV32I-NEXT:    lw a6, 4(a1)
+; RV32I-NEXT:    lw a5, 0(a2)
+; RV32I-NEXT:    lw a7, 0(a1)
+; RV32I-NEXT:    sltu t0, a7, a5
+; RV32I-NEXT:    addi a3, t0, 0
+; RV32I-NEXT:    beq a6, a4, .LBB3_2
+; RV32I-NEXT:  # %bb.1:
+; RV32I-NEXT:    sltu a3, a6, a4
+; RV32I-NEXT:  .LBB3_2:
+; RV32I-NEXT:    sub a4, a6, a4
+; RV32I-NEXT:    sub a4, a4, t0
+; RV32I-NEXT:    sub a5, a7, a5
+; RV32I-NEXT:    sw a5, 0(a0)
+; RV32I-NEXT:    sw a4, 4(a0)
+; RV32I-NEXT:    lw a4, 8(a2)
+; RV32I-NEXT:    lw a5, 8(a1)
+; RV32I-NEXT:    sub a6, a5, a4
+; RV32I-NEXT:    sub a7, a6, a3
+; RV32I-NEXT:    sw a7, 8(a0)
+; RV32I-NEXT:    sltu a4, a5, a4
+; RV32I-NEXT:    lw a2, 12(a2)
+; RV32I-NEXT:    lw a1, 12(a1)
+; RV32I-NEXT:    sub a1, a1, a2
+; RV32I-NEXT:    sub a1, a1, a4
+; RV32I-NEXT:    sltu a2, a6, a3
+; RV32I-NEXT:    sub a1, a1, a2
+; RV32I-NEXT:    sw a1, 12(a0)
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: subc_sube128:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sub a1, a1, a3
+; RV64I-NEXT:    sltu a3, a0, a2
+; RV64I-NEXT:    sub a1, a1, a3
+; RV64I-NEXT:    sub a0, a0, a2
+; RV64I-NEXT:    jalr zero, ra, 0
+  %1 = sub i128 %a, %b
+  ret i128 %1
+}
diff --git a/test/CodeGen/RISCV/alu32.ll b/test/CodeGen/RISCV/alu32.ll
index e7c82181027..106626f07f4 100644
--- a/test/CodeGen/RISCV/alu32.ll
+++ b/test/CodeGen/RISCV/alu32.ll
@@ -1,6 +1,8 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
 ; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
 ; RUN:   | FileCheck %s -check-prefix=RV32I
+; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s \
+; RUN:   | FileCheck %s -check-prefix=RV64I
 
 ; These tests are each targeted at a particular RISC-V ALU instruction. Other
 ; files in this folder exercise LLVM IR instructions that don't directly match a
@@ -13,6 +15,11 @@ define i32 @addi(i32 %a) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    addi a0, a0, 1
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: addi:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    addi a0, a0, 1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = add i32 %a, 1
   ret i32 %1
 }
@@ -22,6 +29,12 @@ define i32 @slti(i32 %a) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    slti a0, a0, 2
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: slti:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    addiw a0, a0, 0
+; RV64I-NEXT:    slti a0, a0, 2
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = icmp slt i32 %a, 2
   %2 = zext i1 %1 to i32
   ret i32 %2
@@ -32,6 +45,19 @@ define i32 @sltiu(i32 %a) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    sltiu a0, a0, 3
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: sltiu:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lui a1, 0
+; RV64I-NEXT:    addiw a2, a1, 0
+; RV64I-NEXT:    slli a2, a2, 32
+; RV64I-NEXT:    addiw a1, a1, -1
+; RV64I-NEXT:    slli a1, a1, 32
+; RV64I-NEXT:    srli a1, a1, 32
+; RV64I-NEXT:    or a1, a2, a1
+; RV64I-NEXT:    and a0, a0, a1
+; RV64I-NEXT:    sltiu a0, a0, 3
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = icmp ult i32 %a, 3
   %2 = zext i1 %1 to i32
   ret i32 %2
@@ -42,6 +68,11 @@ define i32 @xori(i32 %a) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    xori a0, a0, 4
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: xori:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    xori a0, a0, 4
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = xor i32 %a, 4
   ret i32 %1
 }
@@ -51,6 +82,11 @@ define i32 @ori(i32 %a) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    ori a0, a0, 5
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: ori:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    ori a0, a0, 5
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = or i32 %a, 5
   ret i32 %1
 }
@@ -60,6 +96,11 @@ define i32 @andi(i32 %a) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    andi a0, a0, 6
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: andi:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    andi a0, a0, 6
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = and i32 %a, 6
   ret i32 %1
 }
@@ -69,6 +110,11 @@ define i32 @slli(i32 %a) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    slli a0, a0, 7
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: slli:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    slli a0, a0, 7
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = shl i32 %a, 7
   ret i32 %1
 }
@@ -78,6 +124,19 @@ define i32 @srli(i32 %a) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    srli a0, a0, 8
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: srli:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lui a1, 0
+; RV64I-NEXT:    addiw a2, a1, 0
+; RV64I-NEXT:    slli a2, a2, 32
+; RV64I-NEXT:    addiw a1, a1, -256
+; RV64I-NEXT:    slli a1, a1, 32
+; RV64I-NEXT:    srli a1, a1, 32
+; RV64I-NEXT:    or a1, a2, a1
+; RV64I-NEXT:    and a0, a0, a1
+; RV64I-NEXT:    srli a0, a0, 8
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = lshr i32 %a, 8
   ret i32 %1
 }
@@ -87,6 +146,12 @@ define i32 @srai(i32 %a) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    srai a0, a0, 9
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: srai:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    addiw a0, a0, 0
+; RV64I-NEXT:    srai a0, a0, 9
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = ashr i32 %a, 9
   ret i32 %1
 }
@@ -98,6 +163,11 @@ define i32 @add(i32 %a, i32 %b) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    add a0, a0, a1
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: add:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    add a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = add i32 %a, %b
   ret i32 %1
 }
@@ -107,6 +177,11 @@ define i32 @sub(i32 %a, i32 %b) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    sub a0, a0, a1
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: sub:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sub a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = sub i32 %a, %b
   ret i32 %1
 }
@@ -116,6 +191,19 @@ define i32 @sll(i32 %a, i32 %b) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    sll a0, a0, a1
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: sll:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lui a2, 0
+; RV64I-NEXT:    addiw a3, a2, 0
+; RV64I-NEXT:    slli a3, a3, 32
+; RV64I-NEXT:    addiw a2, a2, -1
+; RV64I-NEXT:    slli a2, a2, 32
+; RV64I-NEXT:    srli a2, a2, 32
+; RV64I-NEXT:    or a2, a3, a2
+; RV64I-NEXT:    and a1, a1, a2
+; RV64I-NEXT:    sll a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = shl i32 %a, %b
   ret i32 %1
 }
@@ -125,6 +213,13 @@ define i32 @slt(i32 %a, i32 %b) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    slt a0, a0, a1
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: slt:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    addiw a1, a1, 0
+; RV64I-NEXT:    addiw a0, a0, 0
+; RV64I-NEXT:    slt a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = icmp slt i32 %a, %b
   %2 = zext i1 %1 to i32
   ret i32 %2
@@ -135,6 +230,20 @@ define i32 @sltu(i32 %a, i32 %b) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    sltu a0, a0, a1
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: sltu:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lui a2, 0
+; RV64I-NEXT:    addiw a3, a2, 0
+; RV64I-NEXT:    slli a3, a3, 32
+; RV64I-NEXT:    addiw a2, a2, -1
+; RV64I-NEXT:    slli a2, a2, 32
+; RV64I-NEXT:    srli a2, a2, 32
+; RV64I-NEXT:    or a2, a3, a2
+; RV64I-NEXT:    and a1, a1, a2
+; RV64I-NEXT:    and a0, a0, a2
+; RV64I-NEXT:    sltu a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = icmp ult i32 %a, %b
   %2 = zext i1 %1 to i32
   ret i32 %2
@@ -145,6 +254,11 @@ define i32 @xor(i32 %a, i32 %b) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    xor a0, a0, a1
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: xor:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    xor a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = xor i32 %a, %b
   ret i32 %1
 }
@@ -154,6 +268,19 @@ define i32 @srl(i32 %a, i32 %b) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    srl a0, a0, a1
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: srl:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lui a2, 0
+; RV64I-NEXT:    addiw a3, a2, 0
+; RV64I-NEXT:    slli a3, a3, 32
+; RV64I-NEXT:    addiw a2, a2, -1
+; RV64I-NEXT:    slli a2, a2, 32
+; RV64I-NEXT:    srli a2, a2, 32
+; RV64I-NEXT:    or a2, a3, a2
+; RV64I-NEXT:    and a1, a1, a2
+; RV64I-NEXT:    srlw a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = lshr i32 %a, %b
   ret i32 %1
 }
@@ -163,6 +290,19 @@ define i32 @sra(i32 %a, i32 %b) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    sra a0, a0, a1
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: sra:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lui a2, 0
+; RV64I-NEXT:    addiw a3, a2, 0
+; RV64I-NEXT:    slli a3, a3, 32
+; RV64I-NEXT:    addiw a2, a2, -1
+; RV64I-NEXT:    slli a2, a2, 32
+; RV64I-NEXT:    srli a2, a2, 32
+; RV64I-NEXT:    or a2, a3, a2
+; RV64I-NEXT:    and a1, a1, a2
+; RV64I-NEXT:    sraw a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = ashr i32 %a, %b
   ret i32 %1
 }
@@ -172,6 +312,11 @@ define i32 @or(i32 %a, i32 %b) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    or a0, a0, a1
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: or:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    or a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = or i32 %a, %b
   ret i32 %1
 }
@@ -181,6 +326,11 @@ define i32 @and(i32 %a, i32 %b) nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    and a0, a0, a1
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: and:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    and a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   %1 = and i32 %a, %b
   ret i32 %1
 }
diff --git a/test/CodeGen/RISCV/alu64.ll b/test/CodeGen/RISCV/alu64.ll
new file mode 100644
index 00000000000..2a289387fb3
--- /dev/null
+++ b/test/CodeGen/RISCV/alu64.ll
@@ -0,0 +1,484 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
+; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s \
+; RUN:   | FileCheck %s -check-prefix=RV64I
+; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
+; RUN:   | FileCheck %s -check-prefix=RV32I
+
+; Register-immediate instructions
+
+define i64 @addi(i64 %a) nounwind {
+; RV64I-LABEL: addi:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    addi a0, a0, 1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: addi:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    addi a2, a0, 1
+; RV32I-NEXT:    sltu a0, a2, a0
+; RV32I-NEXT:    add a1, a1, a0
+; RV32I-NEXT:    addi a0, a2, 0
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = add i64 %a, 1
+  ret i64 %1
+}
+
+define i64 @slti(i64 %a) nounwind {
+; RV64I-LABEL: slti:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    slti a0, a0, 2
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: slti:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    beq a1, zero, .LBB1_2
+; RV32I-NEXT:  # %bb.1:
+; RV32I-NEXT:    slti a0, a1, 0
+; RV32I-NEXT:    addi a1, zero, 0
+; RV32I-NEXT:    jalr zero, ra, 0
+; RV32I-NEXT:  .LBB1_2:
+; RV32I-NEXT:    sltiu a0, a0, 2
+; RV32I-NEXT:    addi a1, zero, 0
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = icmp slt i64 %a, 2
+  %2 = zext i1 %1 to i64
+  ret i64 %2
+}
+
+define i64 @sltiu(i64 %a) nounwind {
+; RV64I-LABEL: sltiu:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sltiu a0, a0, 3
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: sltiu:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    beq a1, zero, .LBB2_2
+; RV32I-NEXT:  # %bb.1:
+; RV32I-NEXT:    addi a0, zero, 0
+; RV32I-NEXT:    addi a1, zero, 0
+; RV32I-NEXT:    jalr zero, ra, 0
+; RV32I-NEXT:  .LBB2_2:
+; RV32I-NEXT:    sltiu a0, a0, 3
+; RV32I-NEXT:    addi a1, zero, 0
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = icmp ult i64 %a, 3
+  %2 = zext i1 %1 to i64
+  ret i64 %2
+}
+
+define i64 @xori(i64 %a) nounwind {
+; RV64I-LABEL: xori:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    xori a0, a0, 4
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: xori:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    xori a0, a0, 4
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = xor i64 %a, 4
+  ret i64 %1
+}
+
+define i64 @ori(i64 %a) nounwind {
+; RV64I-LABEL: ori:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    ori a0, a0, 5
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: ori:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    ori a0, a0, 5
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = or i64 %a, 5
+  ret i64 %1
+}
+
+define i64 @andi(i64 %a) nounwind {
+; RV64I-LABEL: andi:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    andi a0, a0, 6
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: andi:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    andi a0, a0, 6
+; RV32I-NEXT:    addi a1, zero, 0
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = and i64 %a, 6
+  ret i64 %1
+}
+
+define i64 @slli(i64 %a) nounwind {
+; RV64I-LABEL: slli:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    slli a0, a0, 7
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: slli:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    slli a1, a1, 7
+; RV32I-NEXT:    srli a2, a0, 25
+; RV32I-NEXT:    or a1, a1, a2
+; RV32I-NEXT:    slli a0, a0, 7
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = shl i64 %a, 7
+  ret i64 %1
+}
+
+define i64 @srli(i64 %a) nounwind {
+; RV64I-LABEL: srli:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    srli a0, a0, 8
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: srli:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    srli a0, a0, 8
+; RV32I-NEXT:    slli a2, a1, 24
+; RV32I-NEXT:    or a0, a0, a2
+; RV32I-NEXT:    srli a1, a1, 8
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = lshr i64 %a, 8
+  ret i64 %1
+}
+
+define i64 @srai(i64 %a) nounwind {
+; RV64I-LABEL: srai:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    srai a0, a0, 9
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: srai:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    srli a0, a0, 9
+; RV32I-NEXT:    slli a2, a1, 23
+; RV32I-NEXT:    or a0, a0, a2
+; RV32I-NEXT:    srai a1, a1, 9
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = ashr i64 %a, 9
+  ret i64 %1
+}
+
+; Register-register instructions
+
+define i64 @add(i64 %a, i64 %b) nounwind {
+; RV64I-LABEL: add:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    add a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: add:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    add a1, a1, a3
+; RV32I-NEXT:    add a2, a0, a2
+; RV32I-NEXT:    sltu a0, a2, a0
+; RV32I-NEXT:    add a1, a1, a0
+; RV32I-NEXT:    addi a0, a2, 0
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = add i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @sub(i64 %a, i64 %b) nounwind {
+; RV64I-LABEL: sub:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sub a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: sub:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    sub a1, a1, a3
+; RV32I-NEXT:    sltu a3, a0, a2
+; RV32I-NEXT:    sub a1, a1, a3
+; RV32I-NEXT:    sub a0, a0, a2
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = sub i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @sll(i64 %a, i64 %b) nounwind {
+; RV64I-LABEL: sll:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sll a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: sll:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a3, %hi(__ashldi3)
+; RV32I-NEXT:    addi a3, a3, %lo(__ashldi3)
+; RV32I-NEXT:    jalr ra, a3, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = shl i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @slt(i64 %a, i64 %b) nounwind {
+; RV64I-LABEL: slt:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    slt a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: slt:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    beq a1, a3, .LBB12_2
+; RV32I-NEXT:  # %bb.1:
+; RV32I-NEXT:    slt a0, a1, a3
+; RV32I-NEXT:    addi a1, zero, 0
+; RV32I-NEXT:    jalr zero, ra, 0
+; RV32I-NEXT:  .LBB12_2:
+; RV32I-NEXT:    sltu a0, a0, a2
+; RV32I-NEXT:    addi a1, zero, 0
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = icmp slt i64 %a, %b
+  %2 = zext i1 %1 to i64
+  ret i64 %2
+}
+
+define i64 @sltu(i64 %a, i64 %b) nounwind {
+; RV64I-LABEL: sltu:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sltu a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: sltu:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    beq a1, a3, .LBB13_2
+; RV32I-NEXT:  # %bb.1:
+; RV32I-NEXT:    sltu a0, a1, a3
+; RV32I-NEXT:    addi a1, zero, 0
+; RV32I-NEXT:    jalr zero, ra, 0
+; RV32I-NEXT:  .LBB13_2:
+; RV32I-NEXT:    sltu a0, a0, a2
+; RV32I-NEXT:    addi a1, zero, 0
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = icmp ult i64 %a, %b
+  %2 = zext i1 %1 to i64
+  ret i64 %2
+}
+
+define i64 @xor(i64 %a, i64 %b) nounwind {
+; RV64I-LABEL: xor:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    xor a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: xor:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    xor a0, a0, a2
+; RV32I-NEXT:    xor a1, a1, a3
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = xor i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @srl(i64 %a, i64 %b) nounwind {
+; RV64I-LABEL: srl:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    srl a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: srl:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a3, %hi(__lshrdi3)
+; RV32I-NEXT:    addi a3, a3, %lo(__lshrdi3)
+; RV32I-NEXT:    jalr ra, a3, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = lshr i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @sra(i64 %a, i64 %b) nounwind {
+; RV64I-LABEL: sra:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sra a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: sra:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a3, %hi(__ashrdi3)
+; RV32I-NEXT:    addi a3, a3, %lo(__ashrdi3)
+; RV32I-NEXT:    jalr ra, a3, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = ashr i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @or(i64 %a, i64 %b) nounwind {
+; RV64I-LABEL: or:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    or a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: or:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    or a0, a0, a2
+; RV32I-NEXT:    or a1, a1, a3
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = or i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @and(i64 %a, i64 %b) nounwind {
+; RV64I-LABEL: and:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    and a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: and:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    and a0, a0, a2
+; RV32I-NEXT:    and a1, a1, a3
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = and i64 %a, %b
+  ret i64 %1
+}
+
+define signext i32 @addiw(i32 signext %a) {
+; RV64I-LABEL: addiw:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    addiw a0, a0, 123
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: addiw:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    addi a0, a0, 123
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = add i32 %a, 123
+  ret i32 %1
+}
+
+define signext i32 @slliw(i32 signext %a) {
+; RV64I-LABEL: slliw:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    slliw a0, a0, 17
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: slliw:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    slli a0, a0, 17
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = shl i32 %a, 17
+  ret i32 %1
+}
+
+define signext i32 @srliw(i32 signext %a) {
+; RV64I-LABEL: srliw:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: srliw:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    jalr zero, ra, 0
+; TODO
+  ret i32 %a
+}
+
+define signext i32 @sraiw(i32 signext %a) {
+; RV64I-LABEL: sraiw:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: sraiw:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    jalr zero, ra, 0
+; TODO
+  ret i32 %a
+}
+
+define signext i32 @sextw(i32 zeroext %a) {
+; RV64I-LABEL: sextw:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    addiw a0, a0, 0
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: sextw:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    jalr zero, ra, 0
+  ret i32 %a
+}
+
+define signext i32 @addw(i32 signext %a, i32 signext %b) {
+; RV64I-LABEL: addw:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    addw a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: addw:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    add a0, a0, a1
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = add i32 %a, %b
+  ret i32 %1
+}
+
+define signext i32 @subw(i32 signext %a, i32 signext %b) {
+; RV64I-LABEL: subw:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    subw a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: subw:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    sub a0, a0, a1
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = sub i32 %a, %b
+  ret i32 %1
+}
+
+define signext i32 @sllw(i32 signext %a, i32 zeroext %b) {
+; RV64I-LABEL: sllw:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sllw a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: sllw:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    sll a0, a0, a1
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = shl i32 %a, %b
+  ret i32 %1
+}
+
+define signext i32 @srlw(i32 signext %a, i32 zeroext %b) {
+; RV64I-LABEL: srlw:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    srlw a0, a0, a1
+; RV64I-NEXT:    addiw a0, a0, 0
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: srlw:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    srl a0, a0, a1
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = lshr i32 %a, %b
+  ret i32 %1
+}
+
+define signext i32 @sraw(i64 %a, i32 zeroext %b) {
+; RV64I-LABEL: sraw:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sraw a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+;
+; RV32I-LABEL: sraw:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    sra a0, a0, a2
+; RV32I-NEXT:    jalr zero, ra, 0
+  %1 = trunc i64 %a to i32
+  %2 = ashr i32 %1, %b
+  ret i32 %2
+}
diff --git a/test/CodeGen/RISCV/imm.ll b/test/CodeGen/RISCV/imm.ll
index ddefa22835a..9487d965f70 100644
--- a/test/CodeGen/RISCV/imm.ll
+++ b/test/CodeGen/RISCV/imm.ll
@@ -1,6 +1,8 @@
 ; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
 ; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
 ; RUN:   | FileCheck %s -check-prefix=RV32I
+; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s \
+; RUN:   | FileCheck %s -check-prefix=RV64I
 
 ; Materializing constants
 
@@ -9,6 +11,11 @@ define i32 @zero() nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    addi a0, zero, 0
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: zero:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    addi a0, zero, 0
+; RV64I-NEXT:    jalr zero, ra, 0
   ret i32 0
 }
 
@@ -17,6 +24,11 @@ define i32 @pos_small() nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    addi a0, zero, 2047
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: pos_small:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    addi a0, zero, 2047
+; RV64I-NEXT:    jalr zero, ra, 0
   ret i32 2047
 }
 
@@ -25,6 +37,17 @@ define i32 @neg_small() nounwind {
 ; RV32I:       # %bb.0:
 ; RV32I-NEXT:    addi a0, zero, -2048
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: neg_small:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lui a0, 0
+; RV64I-NEXT:    addiw a1, a0, 0
+; RV64I-NEXT:    slli a1, a1, 32
+; RV64I-NEXT:    addiw a0, a0, -2048
+; RV64I-NEXT:    slli a0, a0, 32
+; RV64I-NEXT:    srli a0, a0, 32
+; RV64I-NEXT:    or a0, a1, a0
+; RV64I-NEXT:    jalr zero, ra, 0
   ret i32 -2048
 }
 
@@ -34,6 +57,12 @@ define i32 @pos_i32() nounwind {
 ; RV32I-NEXT:    lui a0, 423811
 ; RV32I-NEXT:    addi a0, a0, -1297
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: pos_i32:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lui a0, 423811
+; RV64I-NEXT:    addiw a0, a0, -1297
+; RV64I-NEXT:    jalr zero, ra, 0
   ret i32 1735928559
 }
 
@@ -43,5 +72,40 @@ define i32 @neg_i32() nounwind {
 ; RV32I-NEXT:    lui a0, 912092
 ; RV32I-NEXT:    addi a0, a0, -273
 ; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: neg_i32:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lui a0, 0
+; RV64I-NEXT:    addiw a0, a0, 0
+; RV64I-NEXT:    slli a0, a0, 32
+; RV64I-NEXT:    lui a1, 912092
+; RV64I-NEXT:    addiw a1, a1, -273
+; RV64I-NEXT:    slli a1, a1, 32
+; RV64I-NEXT:    srli a1, a1, 32
+; RV64I-NEXT:    or a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
   ret i32 -559038737
 }
+
+define i64 @pos_imm64() {
+; RV32I-LABEL: pos_imm64:
+; RV32I:       # %bb.0:
+; RV32I-NEXT:    lui a0, 430355
+; RV32I-NEXT:    addi a0, a0, 787
+; RV32I-NEXT:    lui a1, 7018
+; RV32I-NEXT:    addi a1, a1, -1212
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV64I-LABEL: pos_imm64:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lui a0, 7018
+; RV64I-NEXT:    addiw a0, a0, -1212
+; RV64I-NEXT:    slli a0, a0, 32
+; RV64I-NEXT:    lui a1, 430355
+; RV64I-NEXT:    addiw a1, a1, 787
+; RV64I-NEXT:    slli a1, a1, 32
+; RV64I-NEXT:    srli a1, a1, 32
+; RV64I-NEXT:    or a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+  ret i64 123456757922083603
+}
diff --git a/test/CodeGen/RISCV/mem64.ll b/test/CodeGen/RISCV/mem64.ll
new file mode 100644
index 00000000000..9aaf3afc0dc
--- /dev/null
+++ b/test/CodeGen/RISCV/mem64.ll
@@ -0,0 +1,229 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
+; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s \
+; RUN:   | FileCheck -check-prefix=RV64I %s
+
+; Check indexed and unindexed, sext, zext and anyext loads
+
+define i64 @lb(i8 *%a) nounwind {
+; RV64I-LABEL: lb:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lb a1, 0(a0)
+; RV64I-NEXT:    lb a0, 1(a0)
+; RV64I-NEXT:    jalr zero, ra, 0
+  %1 = getelementptr i8, i8* %a, i32 1
+  %2 = load i8, i8* %1
+  %3 = sext i8 %2 to i64
+  ; the unused load will produce an anyext for selection
+  %4 = load volatile i8, i8* %a
+  ret i64 %3
+}
+
+define i64 @lh(i16 *%a) nounwind {
+; RV64I-LABEL: lh:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lh a1, 0(a0)
+; RV64I-NEXT:    lh a0, 4(a0)
+; RV64I-NEXT:    jalr zero, ra, 0
+  %1 = getelementptr i16, i16* %a, i32 2
+  %2 = load i16, i16* %1
+  %3 = sext i16 %2 to i64
+  ; the unused load will produce an anyext for selection
+  %4 = load volatile i16, i16* %a
+  ret i64 %3
+}
+
+define i64 @lw(i32 *%a) nounwind {
+; RV64I-LABEL: lw:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lw a1, 0(a0)
+; RV64I-NEXT:    lw a0, 12(a0)
+; RV64I-NEXT:    jalr zero, ra, 0
+  %1 = getelementptr i32, i32* %a, i32 3
+  %2 = load i32, i32* %1
+  %3 = sext i32 %2 to i64
+  ; the unused load will produce an anyext for selection
+  %4 = load volatile i32, i32* %a
+  ret i64 %3
+}
+
+define i64 @lbu(i8 *%a) nounwind {
+; RV64I-LABEL: lbu:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lbu a1, 0(a0)
+; RV64I-NEXT:    lbu a0, 4(a0)
+; RV64I-NEXT:    add a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+  %1 = getelementptr i8, i8* %a, i32 4
+  %2 = load i8, i8* %1
+  %3 = zext i8 %2 to i64
+  %4 = load volatile i8, i8* %a
+  %5 = zext i8 %4 to i64
+  %6 = add i64 %3, %5
+  ret i64 %6
+}
+
+define i64 @lhu(i16 *%a) nounwind {
+; RV64I-LABEL: lhu:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lhu a1, 0(a0)
+; RV64I-NEXT:    lhu a0, 10(a0)
+; RV64I-NEXT:    add a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+  %1 = getelementptr i16, i16* %a, i32 5
+  %2 = load i16, i16* %1
+  %3 = zext i16 %2 to i64
+  %4 = load volatile i16, i16* %a
+  %5 = zext i16 %4 to i64
+  %6 = add i64 %3, %5
+  ret i64 %6
+}
+
+define i64 @lwu(i32 *%a) nounwind {
+; RV64I-LABEL: lwu:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lwu a1, 0(a0)
+; RV64I-NEXT:    lwu a0, 24(a0)
+; RV64I-NEXT:    add a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+  %1 = getelementptr i32, i32* %a, i32 6
+  %2 = load i32, i32* %1
+  %3 = zext i32 %2 to i64
+  %4 = load volatile i32, i32* %a
+  %5 = zext i32 %4 to i64
+  %6 = add i64 %3, %5
+  ret i64 %6
+}
+
+; Check indexed and unindexed stores
+
+define void @sb(i8 *%a, i8 %b) nounwind {
+; RV64I-LABEL: sb:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sb a1, 7(a0)
+; RV64I-NEXT:    sb a1, 0(a0)
+; RV64I-NEXT:    jalr zero, ra, 0
+  store i8 %b, i8* %a
+  %1 = getelementptr i8, i8* %a, i32 7
+  store i8 %b, i8* %1
+  ret void
+}
+
+define void @sh(i16 *%a, i16 %b) nounwind {
+; RV64I-LABEL: sh:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sh a1, 16(a0)
+; RV64I-NEXT:    sh a1, 0(a0)
+; RV64I-NEXT:    jalr zero, ra, 0
+  store i16 %b, i16* %a
+  %1 = getelementptr i16, i16* %a, i32 8
+  store i16 %b, i16* %1
+  ret void
+}
+
+define void @sw(i32 *%a, i32 %b) nounwind {
+; RV64I-LABEL: sw:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sw a1, 36(a0)
+; RV64I-NEXT:    sw a1, 0(a0)
+; RV64I-NEXT:    jalr zero, ra, 0
+  store i32 %b, i32* %a
+  %1 = getelementptr i32, i32* %a, i32 9
+  store i32 %b, i32* %1
+  ret void
+}
+
+; 64-bit loads and stores
+
+define i64 @ld(i64 *%a) nounwind {
+; RV64I-LABEL: ld:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    ld a1, 0(a0)
+; RV64I-NEXT:    ld a0, 80(a0)
+; RV64I-NEXT:    jalr zero, ra, 0
+  %1 = getelementptr i64, i64* %a, i32 10
+  %2 = load i64, i64* %1
+  %3 = load volatile i64, i64* %a
+  ret i64 %2
+}
+
+define void @sd(i64 *%a, i64 %b) nounwind {
+; RV64I-LABEL: sd:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    sd a1, 88(a0)
+; RV64I-NEXT:    sd a1, 0(a0)
+; RV64I-NEXT:    jalr zero, ra, 0
+  store i64 %b, i64* %a
+  %1 = getelementptr i64, i64* %a, i32 11
+  store i64 %b, i64* %1
+  ret void
+}
+
+; Check load and store to an i1 location
+define i64 @load_sext_zext_anyext_i1(i1 *%a) nounwind {
+  ; sextload i1
+; RV64I-LABEL: load_sext_zext_anyext_i1:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lb a1, 0(a0)
+; RV64I-NEXT:    lbu a1, 1(a0)
+; RV64I-NEXT:    lbu a0, 2(a0)
+; RV64I-NEXT:    sub a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+  %1 = getelementptr i1, i1* %a, i32 1
+  %2 = load i1, i1* %1
+  %3 = sext i1 %2 to i64
+  ; zextload i1
+  %4 = getelementptr i1, i1* %a, i32 2
+  %5 = load i1, i1* %4
+  %6 = zext i1 %5 to i64
+  %7 = add i64 %3, %6
+  ; extload i1 (anyext). Produced as the load is unused.
+  %8 = load volatile i1, i1* %a
+  ret i64 %7
+}
+
+define i16 @load_sext_zext_anyext_i1_i16(i1 *%a) nounwind {
+  ; sextload i1
+; RV64I-LABEL: load_sext_zext_anyext_i1_i16:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lb a1, 0(a0)
+; RV64I-NEXT:    lbu a1, 1(a0)
+; RV64I-NEXT:    lbu a0, 2(a0)
+; RV64I-NEXT:    sub a0, a0, a1
+; RV64I-NEXT:    jalr zero, ra, 0
+  %1 = getelementptr i1, i1* %a, i32 1
+  %2 = load i1, i1* %1
+  %3 = sext i1 %2 to i16
+  ; zextload i1
+  %4 = getelementptr i1, i1* %a, i32 2
+  %5 = load i1, i1* %4
+  %6 = zext i1 %5 to i16
+  %7 = add i16 %3, %6
+  ; extload i1 (anyext). Produced as the load is unused.
+  %8 = load volatile i1, i1* %a
+  ret i16 %7
+}
+
+; Check load and store to a global
+@G = global i64 0
+
+define i64 @ld_sd_global(i64 %a) nounwind {
+; TODO: the addi should be folded in to the ld/sd operations
+; RV64I-LABEL: ld_sd_global:
+; RV64I:       # %bb.0:
+; RV64I-NEXT:    lui a1, %hi(G)
+; RV64I-NEXT:    addi a2, a1, %lo(G)
+; RV64I-NEXT:    ld a1, 0(a2)
+; RV64I-NEXT:    sd a0, 0(a2)
+; RV64I-NEXT:    lui a2, %hi(G+72)
+; RV64I-NEXT:    addi a2, a2, %lo(G+72)
+; RV64I-NEXT:    ld a3, 0(a2)
+; RV64I-NEXT:    sd a0, 0(a2)
+; RV64I-NEXT:    addi a0, a1, 0
+; RV64I-NEXT:    jalr zero, ra, 0
+  %1 = load volatile i64, i64* @G
+  store i64 %a, i64* @G
+  %2 = getelementptr i64, i64* @G, i64 9
+  %3 = load volatile i64, i64* %2
+  store i64 %a, i64* %2
+  ret i64 %1
+}
-- 
2.17.2

